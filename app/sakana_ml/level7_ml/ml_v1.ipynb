{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DDepth_Top',\n",
       " 'DDepth_Bottom',\n",
       " 'DDepth_midPoint',\n",
       " 'NDepth_Top',\n",
       " 'NDepth_Bottom',\n",
       " 'NDepth_midPoint',\n",
       " 'Lat0_Top',\n",
       " 'Lat0_Bottom',\n",
       " 'Lon0_Top',\n",
       " 'Lon0_Bottom',\n",
       " 'max size0_midPoint',\n",
       " 'Behavior0_Bottom',\n",
       " 'Behavior0_Top',\n",
       " 'Behavior0_Average',\n",
       " 'Habitat0_Bottom',\n",
       " 'Habitat0_Top',\n",
       " 'Habitat0_Average',\n",
       " 'Salinity0_Bottom',\n",
       " 'Salinity0_Top',\n",
       " 'Salinity0_Average',\n",
       " 'TemperatureT0_Bottom',\n",
       " 'TemperatureT0_Top',\n",
       " 'TemperatureT0_Average',\n",
       " 'Associate0_Bottom',\n",
       " 'Associate0_Top',\n",
       " 'Associate0_Average',\n",
       " 'Associate_Floating object',\n",
       " 'Associate_Large pelagics']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_total = Path(\"/home/haselab/Documents/tat/Research/app/sakana_ml/level7_ml/Test_dat2_v2.csv\")\n",
    "\n",
    "df = pl.read_csv(path_total, infer_schema_length=1000)\n",
    "# display(df)\n",
    "\n",
    "# x, yともに適用する処理\n",
    "# nullが10以上ある行をdrop\n",
    "# df = df.with_columns(pl.Series([row.count(None) for row in df.iter_rows()]).alias(\"null_count\")).filter(pl.col(\"null_count\") < 10).drop(\"null_count\")\n",
    "\n",
    "# x, yそれぞれに適用する処理\n",
    "x = df.select(df.columns[:-4])\n",
    "x = x.select(~ pl.selectors.ends_with(\"records\"))\n",
    "x = x.select(~ pl.selectors.ends_with(\"ID\"))\n",
    "x = x.select(~ pl.selectors.ends_with(\"Scientific name\"))\n",
    "# x = x.select(~ pl.selectors.contains(\"max size\"))\n",
    "# nullがk行を以下のカラムのみ採用\n",
    "# x = x.select([col_name for col_name in x.columns if df[col_name].null_count() <=100])\n",
    "x = x.select([col_name for col_name in x.columns if df[col_name].null_count() <=100 or \"associate\" in col_name.lower()])\n",
    "\n",
    "# df = df.select([col_name for col_name in df.columns if df[col_name].null_count() <= 100 or (df.get_column_index(col_name) - df.width) >= -4])\n",
    "\n",
    "\n",
    "ys = df.select(df.columns[-4:])\n",
    "ys = ys.select(ys.columns[-3:])\n",
    "ys = ys.select(ys.columns[0])\n",
    "\n",
    "\n",
    "# # BlankをNに置き換え\n",
    "# ys = ys.with_columns(\n",
    "#     pl.when(pl.all().is_null())\n",
    "#     .then(pl.lit(\"N\"))\n",
    "#     .otherwise(pl.all())\n",
    "#     .name.keep()\n",
    "# )\n",
    "\n",
    "# NとYを0, 1に置き換え\n",
    "ys = ys.with_columns(\n",
    "    pl.when(pl.all() == \"N\")\n",
    "    .then(pl.lit(2))\n",
    "    .when(pl.all() == \"Y\")\n",
    "    .then(pl.lit(1))\n",
    "    .otherwise(pl.lit(0))\n",
    "    .name.keep()\n",
    ")\n",
    "\n",
    "# display(x)\n",
    "# display(ys)\n",
    "\n",
    "display(x.columns)\n",
    "# display(ys.columns)\n",
    "\n",
    "# ys.write_csv(\"./tmp.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, log_loss\n",
    "\n",
    "# テストデータの評価\n",
    "class Result:\n",
    "    def __init__(self, val, proba):\n",
    "        self.val = val\n",
    "        self.proba = proba\n",
    "        self.pred = proba.argmax(axis=1)\n",
    "\n",
    "    def acc(self):\n",
    "        return accuracy_score(self.val, self.pred)\n",
    "\n",
    "    def f1(self):\n",
    "        return f1_score(self.val, self.pred, average=\"macro\") \n",
    "\n",
    "    def auc(self):\n",
    "        return roc_auc_score(self.val, self.pred)\n",
    "    \n",
    "    def logloss(self):\n",
    "        return log_loss(self.val, self.proba)\n",
    "\n",
    "    def cm(self):\n",
    "        return confusion_matrix(self.val, self.pred)\n",
    "\n",
    "                # r = Result(y_val, y_proba)\n",
    "                # s.add_met(\"acc\", r.acc())\n",
    "                # s.add_met(\"f1\", r.f1())\n",
    "                # s.add_met(\"auc\", r.auc())\n",
    "                # s.add_met(\"logloss\", r.logloss())\n",
    "                # s.add_met(\"cm\", r.cm(), cm=True)\n",
    "\n",
    "    # def acc(self):\n",
    "    #     return getattr(self, \"acc_tmp\", self.acc_tmp := accuracy_score(self.val, self.pred))\n",
    "\n",
    "    # def acc(self):\n",
    "    #     attrname = \"acc_tmp\"\n",
    "    #     try:\n",
    "    #         return getattr(self, attrname)\n",
    "    #     except:\n",
    "\n",
    "    #         score = accuracy_score(self.val, self.pred)\n",
    "\n",
    "    #         setattr(self, attrname, score)\n",
    "    #         return getattr(self, attrname)\n",
    "\n",
    "\n",
    "class Scores:\n",
    "    def __init__(self):\n",
    "        self.score_dict = {}\n",
    "        self.cm_dict = {}\n",
    "        self.proba_dict = {}\n",
    "        \n",
    "    def add_met(self, met_name, met_val, cm=False, proba=False):\n",
    "        # if cm:\n",
    "        #     ob_dict = self.cm_dict\n",
    "        # elif proba:\n",
    "        #     ob_dict = self.proba_dict\n",
    "        # else:\n",
    "        #     ob_dict = self.score_dict\n",
    "\n",
    "        # if ob_dict.get(met_name) is None:\n",
    "        #     ob_dict[met_name] = [met_val]\n",
    "        # else:\n",
    "        #     ob_dict[met_name].append(met_val)\n",
    "\n",
    "        if cm:\n",
    "            self.add_dict(self.cm_dict, met_name, met_val)\n",
    "        elif proba:\n",
    "            self.add_dict(self.proba_dict, met_name, met_val)\n",
    "        else:\n",
    "            self.add_dict(self.score_dict, met_name, met_val)\n",
    "\n",
    "    # def ave_met(self, met_name):\n",
    "    #     met_dict = self.score_dict | self.cm_dict | self.proba_dict\n",
    "\n",
    "    #     return met_dict[met_name]\n",
    "    \n",
    "    def add_dict(self, dict_, met_name, met_val):\n",
    "        if dict_.get(met_name) is None:\n",
    "            dict_[met_name] = [met_val]\n",
    "        else:\n",
    "            dict_[met_name].append(met_val)\n",
    "\n",
    "    def ave_mets(self):\n",
    "        s = Scores()\n",
    "        for k, v in self.score_dict.items():\n",
    "            s.add_met(k, sum(v) / len(v))\n",
    "        for k, v in self.cm_dict.items():\n",
    "            s.add_met(k, sum(v) / len(v), cm=True)\n",
    "        for k, v in self.proba_dict.items():\n",
    "            s.add_met(k, np.stack(v).mean(axis=0), proba=True)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def fold_mets(self):\n",
    "        s = Scores()\n",
    "        for k, v in self.score_dict.items():\n",
    "            s.add_met(k, sum(v) / len(v))\n",
    "        for k, v in self.cm_dict.items():\n",
    "            s.add_met(k, sum(v), cm=True)\n",
    "        for k, v in self.proba_dict.items():\n",
    "            s.add_met(k, np.stack(v).mean(axis=0), proba=True)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def print_mets(self):\n",
    "        met_dict = self.score_dict | self.cm_dict | self.proba_dict\n",
    "        for k, v in met_dict.items():\n",
    "            print(f\"{k:8} = {v[0]}\")\n",
    "            \n",
    "    def __or__(self, other):\n",
    "        for k, v in other.score_dict.items():\n",
    "            self.add_met(k, *v)\n",
    "        for k, v in other.cm_dict.items():\n",
    "            self.add_met(k, *v, cm=True)\n",
    "        for k, v in other.proba_dict.items():\n",
    "            self.add_met(k, *v, proba=True)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        met_dict = self.score_dict | self.cm_dict | self.proba_dict\n",
    "        \n",
    "        return met_dict[index]\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Using scikit-learn API\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, KFold\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "# Using scikit-learn API\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # num_leaves = trial.suggest_uniform('num_leaves', 25, 36)\n",
    "    # max_depth = trial.suggest_uniform('num_leaves', 25, 36)\n",
    "\n",
    "    params = {\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 25, 60), # 31\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 7), # 10\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 1.0), # 1.0\n",
    "        'objective': 'binary',\n",
    "        \"n_estimators\": 100,\n",
    "        \"learning_rate\": 0.1,\n",
    "\n",
    "        \"task\": \"train\",\n",
    "        'metric':'binary_logloss',\n",
    "        'seed': 0,\n",
    "        'verbosity': -1,\n",
    "    }\n",
    "\n",
    "    df_res_prob = None\n",
    "\n",
    "    for y in ys:\n",
    "\n",
    "        f = Scores()\n",
    "        for ri in range(5):\n",
    "            kf = KFold(n_splits=4, shuffle=True, random_state=ri)\n",
    "            proba = [None for i in range(len(x))]\n",
    "            \n",
    "            # proba管理用に一時的に\n",
    "            met_dict = dict()\n",
    "\n",
    "            s = Scores()\n",
    "            for fold, (train_indices, val_indices) in enumerate(kf.split(x)):\n",
    "                x_train, x_val = x[train_indices], x[val_indices]\n",
    "                y_train, y_val = y[train_indices], y[val_indices]\n",
    "                \n",
    "                # x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.75, random_state=0)\n",
    "                # print(f\"{len(x_train)=}, {len(x_val)=}\")\n",
    "\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(x_train, y_train)\n",
    "\n",
    "                # y_pred_tmp = model.predict(x_val)\n",
    "                y_proba = model.predict_proba(x_val)\n",
    "                y_pred = y_proba.argmax(axis=1)\n",
    "\n",
    "                r = Result(y_val, y_proba)\n",
    "                s.add_met(\"acc\", r.acc())\n",
    "                s.add_met(\"f1\", r.f1())\n",
    "                s.add_met(\"auc\", r.auc())\n",
    "                s.add_met(\"logloss\", r.logloss())\n",
    "                s.add_met(\"cm\", r.cm(), cm=True)\n",
    "                # s.add_met(\"proba\", r.proba, proba=True)\n",
    "\n",
    "                for i, idx in enumerate(val_indices):\n",
    "                    proba[idx] = y_proba[i]\n",
    "            proba = np.stack(proba)\n",
    "\n",
    "            if met_dict.get(\"proba\") is None:\n",
    "                met_dict[\"proba\"] = [proba]\n",
    "            else:\n",
    "                met_dict[\"proba\"].append(proba)\n",
    "\n",
    "            f |= s.fold_mets()\n",
    "        # print(model.feature_importances_)\n",
    "\n",
    "        a = f.ave_mets()\n",
    "        # print(a.score_dict[\"f1\"])\n",
    "        print(a.score_dict[\"logloss\"])\n",
    "\n",
    "        cm = a.cm_dict[\"cm\"][0]\n",
    "        # a.print_mets()\n",
    "\n",
    "        # fig, ax = plt.subplots()\n",
    "        # fig.set_figwidth(3)\n",
    "        # fig.set_figheight(2.25)\n",
    "\n",
    "        # ax = sns.heatmap(cm, annot=True, cbar=True, square=True, fmt=\".0f\", cmap=\"Blues_r\", xticklabels=list(range(cm.shape[0])), yticklabels=list(range(cm.shape[1])))\n",
    "        # ax.set_xlabel(\"pred_label\")\n",
    "        # ax.set_ylabel(\"true_label\")\n",
    "        \n",
    "        se_proba = pl.Series(proba[:, 1]).rename(y.name + \"_prob\")\n",
    "        if df_res_prob is None:\n",
    "            df_res_prob = se_proba.to_frame()\n",
    "        else:\n",
    "            df_res_prob = df_res_prob.with_columns(se_proba)\n",
    "            \n",
    "            \n",
    "# display(df_res_prob)\n",
    "\n",
    "    df_res = df_res_prob.with_columns(\n",
    "        pl.when(pl.all() < 0.5)\n",
    "        .then(pl.lit(\"N\"))\n",
    "        .otherwise(pl.lit(\"Y\"))\n",
    "        .name\n",
    "        .map(lambda x: x[:-5] + \"_pred\")\n",
    "    )\n",
    "\n",
    "# display(df_pred)\n",
    "\n",
    "# print(x.columns)\n",
    "# print(model.feature_importances_)\n",
    "\n",
    "\n",
    "    # cf = {c: f for c, f in zip(x.columns, model.feature_importances_)}\n",
    "    # cf = {k: v for k, v in sorted(cf.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    # for c, f in cf.items():\n",
    "        # print(f\"{c:30}:{f}\")\n",
    "\n",
    "    return a.score_dict[\"f1\"]\n",
    "    # return a.score_dict[\"logloss\"]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    study = optuna.create_study(\n",
    "        storage=\"sqlite:///db.sqlite3\",\n",
    "        study_name=\"Nyaaaaaaaaaaaa\",\n",
    "        direction='minimize',\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using scikit-learn API\n",
    "# from sklearn.model_selection import train_test_split, KFold\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, log_loss\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "\n",
    "# params = {\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"num_leaves\": 31,\n",
    "#     \"max_depth\": 10,\n",
    "#     'objective': 'binary',\n",
    "#     \"n_estimators\": 100,\n",
    "#     \"learning_rate\": 0.1,\n",
    "\n",
    "#     \"task\": \"train\",\n",
    "#     'metric':'binary_logloss',\n",
    "#     'seed': 0,\n",
    "#     'verbosity': -1,\n",
    "# }\n",
    "\n",
    "# df_res_prob = None\n",
    "\n",
    "# for y in ys:\n",
    "\n",
    "#     f = Scores()\n",
    "#     for ri in range(5):\n",
    "#         kf = KFold(n_splits=4, shuffle=True, random_state=ri)\n",
    "#         proba = [None for i in range(len(x))]\n",
    "        \n",
    "#         # proba管理用に一時的に\n",
    "#         met_dict = dict()\n",
    "\n",
    "#         s = Scores()\n",
    "#         for fold, (train_indices, val_indices) in enumerate(kf.split(x)):\n",
    "#             x_train, x_val = x[train_indices], x[val_indices]\n",
    "#             y_train, y_val = y[train_indices], y[val_indices]\n",
    "            \n",
    "#             # x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.75, random_state=0)\n",
    "#             # print(f\"{len(x_train)=}, {len(x_val)=}\")\n",
    "\n",
    "#             model = lgb.LGBMClassifier(**params)\n",
    "#             model.fit(x_train, y_train)\n",
    "\n",
    "#             # y_pred_tmp = model.predict(x_val)\n",
    "#             y_proba = model.predict_proba(x_val)\n",
    "#             y_pred = y_proba.argmax(axis=1)\n",
    "\n",
    "#             r = Result(y_val, y_proba)\n",
    "#             s.add_met(\"acc\", r.acc())\n",
    "#             s.add_met(\"f1\", r.f1())\n",
    "#             s.add_met(\"auc\", r.auc())\n",
    "#             s.add_met(\"logloss\", r.logloss())\n",
    "#             s.add_met(\"cm\", r.cm(), cm=True)\n",
    "#             # s.add_met(\"proba\", r.proba, proba=True)\n",
    "\n",
    "#             for i, idx in enumerate(val_indices):\n",
    "#                 proba[idx] = y_proba[i]\n",
    "#         proba = np.stack(proba)\n",
    "\n",
    "#         if met_dict.get(\"proba\") is None:\n",
    "#             met_dict[\"proba\"] = [proba]\n",
    "#         else:\n",
    "#             met_dict[\"proba\"].append(proba)\n",
    "\n",
    "#         f |= s.fold_mets()\n",
    "#     # print(model.feature_importances_)\n",
    "\n",
    "#     a = f.ave_mets()\n",
    "#     print(a.score_dict[\"logloss\"])\n",
    "\n",
    "#     cm = a.cm_dict[\"cm\"][0]\n",
    "#     # a.print_mets()\n",
    "\n",
    "#     fig, ax = plt.subplots()\n",
    "#     fig.set_figwidth(3)\n",
    "#     fig.set_figheight(2.25)\n",
    "\n",
    "#     ax = sns.heatmap(cm, annot=True, cbar=True, square=True, fmt=\".0f\", cmap=\"Blues_r\", xticklabels=list(range(cm.shape[0])), yticklabels=list(range(cm.shape[1])))\n",
    "#     ax.set_xlabel(\"pred_label\")\n",
    "#     ax.set_ylabel(\"true_label\")\n",
    "    \n",
    "#     se_proba = pl.Series(proba[:, 1]).rename(y.name + \"_prob\")\n",
    "#     if df_res_prob is None:\n",
    "#         df_res_prob = se_proba.to_frame()\n",
    "#     else:\n",
    "#         df_res_prob = df_res_prob.with_columns(se_proba)\n",
    "        \n",
    "        \n",
    "# # display(df_res_prob)\n",
    "\n",
    "# df_res = df_res_prob.with_columns(\n",
    "#     pl.when(pl.all() < 0.5)\n",
    "#     .then(pl.lit(\"N\"))\n",
    "#     .otherwise(pl.lit(\"Y\"))\n",
    "#     .name\n",
    "#     .map(lambda x: x[:-5] + \"_pred\")\n",
    "# )\n",
    "\n",
    "# # display(df_pred)\n",
    "\n",
    "# # print(x.columns)\n",
    "# # print(model.feature_importances_)\n",
    "\n",
    "# cf = {c: f for c, f in zip(x.columns, model.feature_importances_)}\n",
    "# cf = {k: v for k, v in sorted(cf.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "# pass\n",
    "\n",
    "# # for c, f in cf.items():\n",
    "#     # print(f\"{c:30}:{f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
